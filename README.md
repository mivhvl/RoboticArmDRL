# ğŸ¤– Robot Arm Learning Task â€“ FEUP

This project involves training a robotic arm using **Reinforcement Learning** (Proximal Policy Optimization â€“ PPO) with the `robosuite` simulator to grasp a cube and move it to another location (another table surface). Developed as part of a robotics and AI course at **FEUP**.

![Robot Arm Simulation](robot_arm.jpg)

---

## ğŸ“ Project Structure
```
Root/
â”œâ”€â”€ DLR/
â”‚   â””â”€â”€ network.py              # PPO agent and network
â”œâ”€â”€ env/
â”‚   â””â”€â”€ PickMove.py             # Custom environment
â”œâ”€â”€ models/                     # Saved models (checkpoints, final_model.pth)
â”œâ”€â”€ run.py                      # Training execution script
â”œâ”€â”€ check.py                    # Model evaluation / debugging
â”œâ”€â”€ requirements.txt            # Dependencies
â”œâ”€â”€ README.txt                  # This file
â””â”€â”€ .gitignore                  # Git ignore list
```

## ğŸ“¦ Dependencies

To install all required packages:
```
pip install -r requirements.txt
```
Minimum required packages:
- robosuite==1.4.0
- torch
- numpy
- matplotlib

**Important**: To install `robosuite` and `robosuite_models` correctly, please follow the official installation steps here:

ğŸ‘‰ https://robosuite.ai/docs/installation.html

This ensures that MuJoCo and all simulation dependencies are configured properly.

---



1. Train the agent:
```
   python run.py
```

2. Optional: Evaluate model performance:
```
   python check.py
```
3. Output graphs:
   - reward_curve.png â€“ Total reward per episode
   - loss_curve.png â€“ PPO training loss
   - value_vs_return.png â€“ Critic value estimation accuracy
   - reward_components.png â€“ Optional: tracks reach, grasp, and place rewards

4. Model checkpoints are saved in the `models/` directory:
   - best_model.pth
   - final_model.pth
   - checkpoint_XXX.pth (if periodic saves are enabled)

To resume training from a checkpoint:

agent.load_model('models/best_model.pth')

---

## ğŸ“‚ Source Code Description

- `env/PickMove.py` â€“ Custom environment with reward shaping and task logic.
- `DLR/network.py` â€“ PPOAgent class with actor-critic networks and training.
- `run.py` â€“ Main training loop.
- `check.py` â€“ Evaluation and rendering (optional).
- `requirements.txt` â€“ Dependency list.

---

## ğŸ‘¥ Authors & Institution

- Author(s): JoÃ£o A. C. Viveiros, Santiago Romero Pineda, GonÃ§alo P. N. de Pinho, MichaÅ‚ Dawid Kowalski
- Course: Deep Learning for Robotics
- Institution: FEUP â€“ Faculdade de Engenharia da Universidade do Porto
- Date: June 2025

---

## ğŸ“˜ Notes

- You can fine-tune a trained model by adjusting the reward function in `PickMove.py`.
- Logs and reward components help diagnose grasping and placing performance.
  
---
